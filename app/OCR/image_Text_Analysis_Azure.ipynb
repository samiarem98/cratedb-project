{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script Documentation: Image and Text Processing Pipeline with Azure OCR, OpenAI, and CrateDB\n",
    "\n",
    "1. Overview\n",
    "\n",
    "This script provides a complete pipeline for processing documents and images, extracting meaningful text and keywords, storing data in CrateDB, and using Azure OCR and OpenAI's API for generating embeddings and chatbot responses. The key functionality includes:\n",
    "\n",
    "Extracting images from PDFs\n",
    "\n",
    "Performing OCR (Optical Character Recognition) using Azure's Computer Vision API\n",
    "\n",
    "Generating embeddings for text using OpenAI's API\n",
    "\n",
    "Storing extracted data in CrateDB\n",
    "\n",
    "Using KNN (K-Nearest Neighbors) search in CrateDB to find relevant context for a user question\n",
    "\n",
    "Generating responses to user queries based on document content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from rake_nltk import Rake\n",
    "from crate import client\n",
    "import fitz  \n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "AZURE_ENDPOINT = os.environ['AZURE_ENDPOINT']\n",
    "AZURE_SUBSCRIPTION_KEY = os.environ['AZURE_SUBSCRIPTION_KEY']\n",
    "CRATEDB_PW = os.environ['CRATEDB_PW']\n",
    "CRATEDB_URI = os.environ['CRATEDB_URI']\n",
    "USERNAME = \"admin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sami.arem\\AppData\\Local\\Temp\\ipykernel_26840\\3190570084.py:2: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "computervision_client = ComputerVisionClient(AZURE_ENDPOINT, CognitiveServicesCredentials(AZURE_SUBSCRIPTION_KEY))\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to extract images from a PDF using PyMuPDF\n",
    "def extract_images_with_pymupdf(pdf_path, output_folder):\n",
    "    \"\"\"Extract images from a PDF using PyMuPDF and save them.\"\"\"\n",
    "    pdf_document = fitz.open(pdf_path)  # Open the PDF document\n",
    "    image_count = 0\n",
    "\n",
    "    for page_number in range(len(pdf_document)):\n",
    "        page = pdf_document[page_number]  # Get the page\n",
    "        image_list = page.get_images(full=True)  # Get list of images on the page\n",
    "\n",
    "        for img_index, img in enumerate(image_list):\n",
    "            xref = img[0]  # Image reference number\n",
    "            base_image = pdf_document.extract_image(xref)  # Extract the image\n",
    "            image_bytes = base_image[\"image\"]  # Get the image bytes\n",
    "            image_ext = base_image[\"ext\"]  # Get the image file extension (e.g., png, jpeg)\n",
    "\n",
    "            # Create an image file name and save the image\n",
    "            image_filename = f\"page_{page_number + 1}_img_{img_index + 1}.{image_ext}\"\n",
    "            image_path = os.path.join(output_folder, image_filename)\n",
    "            with open(image_path, \"wb\") as image_file:\n",
    "                image_file.write(image_bytes)\n",
    "\n",
    "            print(f\"Saved image: {image_filename}\")\n",
    "            image_count += 1\n",
    "\n",
    "    pdf_document.close()  # Close the PDF document\n",
    "    return image_count\n",
    "\n",
    "# CrateDB connection setup\n",
    "def get_crate_connection():\n",
    "    \"\"\"\n",
    "    Establish a connection to CrateDB.\n",
    "    \"\"\"\n",
    "    return client.connect(CRATEDB_URI, username=USERNAME, password=CRATEDB_PW, verify_ssl_cert=True)\n",
    "\n",
    "# Function to generate tags\n",
    "def generate_tags(keywords):\n",
    "    \"\"\"\n",
    "    Generate tags from the most significant keywords.\n",
    "    \"\"\"\n",
    "    return keywords[:5] if len(keywords) > 5 else keywords\n",
    "\n",
    "# Function to extract the page number from the image name\n",
    "def extract_page_number(image_name):\n",
    "    \"\"\"\n",
    "    Extract the page number from the image file name.\n",
    "    Assumes file names follow the pattern 'page_<number>*.png'.\n",
    "    \"\"\"\n",
    "    match = re.search(r'page_(\\d+)', image_name)\n",
    "    if match:\n",
    "        return int(match.group(1))  # Return the number after 'page_'\n",
    "    return None  # Return None if no match is found\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleans the extracted text.\"\"\"\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "# Function to extract keywords using RAKE\n",
    "def extract_keywords(text):\n",
    "    \"\"\"Extract keywords using RAKE.\"\"\"\n",
    "    rake = Rake()\n",
    "    rake.extract_keywords_from_text(text)\n",
    "    return rake.get_ranked_phrases()\n",
    "\n",
    "# Function to filter and prioritize keywords\n",
    "def filter_and_prioritize_keywords(keywords):\n",
    "    \"\"\"Filters and prioritizes keywords, removing punctuation and non-relevant words.\"\"\"\n",
    "    filtered_keywords = {\n",
    "        keyword.strip().lower()\n",
    "        for keyword in keywords\n",
    "        if len(keyword.split()) >= 2 and len(keyword) > 5 and\n",
    "        not any(punc in keyword for punc in [\"(\", \")\", \"+\", \"-\", \"=\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\", \"the\", \"yes\", \"no\"]) \n",
    "    }\n",
    "    return sorted(filtered_keywords, key=lambda k: -len(k))  # Sort by length (longer phrases first))\n",
    "\n",
    "# Function to analyze an image with Azure OCR and extract keywords\n",
    "def analyze_image_with_azure(image_path):\n",
    "    \"\"\"Extract text and refine keywords.\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        ocr_result = computervision_client.read_in_stream(image_file, raw=True)\n",
    "    operation_location = ocr_result.headers[\"Operation-Location\"]\n",
    "    operation_id = operation_location.split(\"/\")[-1]\n",
    "\n",
    "    while True:\n",
    "        result = computervision_client.get_read_result(operation_id)\n",
    "        if result.status not in [\"notStarted\", \"running\"]:\n",
    "            break\n",
    "        print(\"Waiting for Azure OCR results...\")\n",
    "        time.sleep(5)\n",
    "\n",
    "    if result.status == \"succeeded\":\n",
    "        extracted_text = \"\"\n",
    "        for page in result.analyze_result.read_results:\n",
    "            for line in page.lines:\n",
    "                extracted_text += line.text + \" \"\n",
    "        clean_extracted_text = clean_text(extracted_text)\n",
    "        raw_keywords = extract_keywords(clean_extracted_text)\n",
    "\n",
    "        # If no keywords, attempt single-word extraction\n",
    "        if not raw_keywords:\n",
    "            raw_keywords = clean_extracted_text.split()\n",
    "\n",
    "        # Filter and prioritize keywords (without important_terms)\n",
    "        final_keywords = filter_and_prioritize_keywords(raw_keywords)\n",
    "        return clean_extracted_text, final_keywords\n",
    "\n",
    "    return \"\", []\n",
    "# Function to generate image description\n",
    "def generate_image_description(image_path):\n",
    "    \"\"\"\n",
    "    Generate a description for an image using Azure's Computer Vision API or a similar service.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            description_result = computervision_client.describe_image_in_stream(image_file)\n",
    "\n",
    "        if description_result.captions:\n",
    "            # Select the caption with the highest confidence\n",
    "            description = max(description_result.captions, key=lambda c: c.confidence).text\n",
    "            print(f\"Generated image description: {description}\")\n",
    "            return description\n",
    "        else:\n",
    "            print(\"No description could be generated for the image.\")\n",
    "            return \"No description available.\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating image description for {image_path}: {e}\")\n",
    "        return \"Error generating description.\"\n",
    "\n",
    "# Function to store data in CrateDB\n",
    "def store_data_to_cratedb(page_number, image_name, extracted_text, keywords, tags, embedding_text, embedding_keywords, image_description):\n",
    "    \"\"\"\n",
    "    Store extracted text, keywords, tags, embeddings, and image description into CrateDB.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = get_crate_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Ensure the table exists\n",
    "        create_table_query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS text_data (\n",
    "            page_number INT,\n",
    "            image_name TEXT,\n",
    "            text TEXT,\n",
    "            keywords TEXT,\n",
    "            tags TEXT,\n",
    "            embedding_text FLOAT_VECTOR(1536),  -- Embedding for the text\n",
    "            embedding_keywords FLOAT_VECTOR(1536),\n",
    "            image_description TEXT  -- Description of the image\n",
    "        );\n",
    "        \"\"\"\n",
    "        cursor.execute(create_table_query)\n",
    "        conn.commit()\n",
    "\n",
    "        # Insert data\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO text_data (page_number, image_name, text, keywords, tags, embedding_text, embedding_keywords, image_description)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?);\n",
    "        \"\"\"\n",
    "        cursor.execute(\n",
    "            insert_query,\n",
    "            (\n",
    "                page_number,\n",
    "                image_name,\n",
    "                extracted_text,\n",
    "                \", \".join(keywords),  # Convert keywords to a single string\n",
    "                \", \".join(tags),  # Convert tags to a single string\n",
    "                embedding_text,  # Embedding vector for the text\n",
    "                embedding_keywords,  # Embedding vector for the keywords\n",
    "                image_description  # Description of the image\n",
    "            ),\n",
    "        )\n",
    "        conn.commit()\n",
    "        print(f\"Data from {image_name} stored successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error storing data to CrateDB: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting images from PDF...\n",
      "Saved image: page_1_img_1.png\n",
      "Saved image: page_4_img_1.png\n",
      "Saved image: page_5_img_1.png\n",
      "Saved image: page_6_img_1.png\n",
      "Saved image: page_6_img_2.png\n",
      "Saved image: page_6_img_3.png\n",
      "Saved image: page_8_img_1.png\n",
      "Saved image: page_9_img_1.png\n",
      "Saved image: page_10_img_1.png\n",
      "Saved image: page_10_img_2.png\n",
      "Saved image: page_11_img_1.png\n",
      "Saved image: page_12_img_1.png\n",
      "Saved image: page_13_img_1.png\n",
      "Saved image: page_13_img_2.png\n",
      "Saved image: page_14_img_1.png\n",
      "Saved image: page_16_img_1.png\n",
      "Saved image: page_16_img_2.png\n",
      "Saved image: page_17_img_1.png\n",
      "Saved image: page_17_img_2.png\n",
      "Saved image: page_19_img_1.png\n",
      "Saved image: page_21_img_1.png\n",
      "Saved image: page_21_img_2.png\n",
      "Saved image: page_24_img_1.png\n",
      "Saved image: page_24_img_2.jpeg\n",
      "Saved image: page_25_img_1.png\n",
      "Saved image: page_32_img_1.png\n",
      "Saved image: page_33_img_1.png\n",
      "Saved image: page_34_img_1.png\n",
      "Saved image: page_38_img_1.png\n",
      "Saved image: page_40_img_1.png\n",
      "Saved image: page_41_img_1.png\n",
      "Saved image: page_42_img_1.png\n",
      "Saved image: page_47_img_1.png\n",
      "Saved image: page_47_img_2.png\n",
      "Saved image: page_47_img_3.png\n",
      "Saved image: page_49_img_1.png\n",
      "Saved image: page_49_img_2.png\n",
      "Saved image: page_54_img_1.png\n",
      "Saved image: page_54_img_2.png\n",
      "Saved image: page_55_img_1.png\n",
      "Saved image: page_55_img_2.png\n",
      "Saved image: page_56_img_1.png\n",
      "Saved image: page_59_img_1.png\n",
      "Saved image: page_60_img_1.png\n",
      "Saved image: page_60_img_2.png\n",
      "Saved image: page_60_img_3.png\n",
      "Saved image: page_62_img_1.png\n",
      "Saved image: page_62_img_2.png\n",
      "Saved image: page_63_img_1.png\n",
      "Saved image: page_64_img_1.png\n",
      "Saved image: page_64_img_2.png\n",
      "Saved image: page_65_img_1.png\n",
      "Extracted 52 images from the PDF.\n",
      "\n",
      "Processing image: marketecture.png\n",
      "Could not extract page number from marketecture.png. Skipping this image.\n",
      "\n",
      "Processing image: page_10_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_10_img_1.png (Page 10):\n",
      "Access mode Visible to user Unity Catalog Supported dropdown support languages Single user Always Yes Python, SQL, Scala, R Shared Always (Premium plan required) Yes Python (DBR 11.1+), SQL No isolation Can be hidden by enforcing user isolation in the admin Python, SQL, shared console or configuring account-level settings No Scala, R Only shown for existing clusters without access modes Custom (i.e. legacy cluster modes, Standard or High No Python, SQL, Concurrency); not an option for creating new clusters. Scala, R\n",
      "Extracted Keywords from page_10_img_1.png (Page 10):\n",
      "existing clusters without access modes custom, enforcing user isolation, creating new clusters, premium plan required, legacy cluster modes, configuring account, access mode visible, r shared always, level settings, shared console, admin python\n",
      "Generated Tags: existing clusters without access modes custom, enforcing user isolation, creating new clusters, premium plan required, legacy cluster modes\n",
      "Generated image description: table\n",
      "Data from page_10_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_10_img_2.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_10_img_2.png (Page 10):\n",
      "Worker type Min workers Max workers Standard_DS3_v2 14 GB Memory, 4 Cores Iv 2 8 Spot instances Driver type Same as worker 14 GB Memory, 4 Cores v Enable autoscaling\n",
      "Extracted Keywords from page_10_img_2.png (Page 10):\n",
      "\n",
      "Generated Tags: \n",
      "Generated image description: graphical user interface, text, application\n",
      "Data from page_10_img_2.png stored successfully.\n",
      "\n",
      "Processing image: page_11_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_11_img_1.png (Page 11):\n",
      "Summary 1 Driver 14 GB Memory, 4 Cores Runtime 13.3.x-scala2.12 Standard_DS3_v2 0.75 DBU/h\n",
      "Extracted Keywords from page_11_img_1.png (Page 11):\n",
      "\n",
      "Generated Tags: \n",
      "Generated image description: graphical user interface, text, application\n",
      "Data from page_11_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_12_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_12_img_1.png (Page 12):\n",
      "No Permissions Can Attach To Can Restart Can Manage Attach notebook V View Spark Ul, cluster metrics, driver logs V Start, restart, terminate V Edit V Attach library Resize V Change permissions\n",
      "Extracted Keywords from page_12_img_1.png (Page 12):\n",
      "terminate v edit v attach library resize v change permissions, driver logs v start, cluster metrics\n",
      "Generated Tags: terminate v edit v attach library resize v change permissions, driver logs v start, cluster metrics\n",
      "Generated image description: table\n",
      "Data from page_12_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_13_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_13_img_1.png (Page 13):\n",
      "Utility Description Example fs Manipulates the Databricks filesystem (DBFS) from the console dbutils.fs.ls() Provides utilities for leveraging secrets within secrets notebooks dbutils. secrets.get() notebook Utilities for the control flow of a notebook dbutils.notebook.run() widgets Methods to create and get bound value of input widgets inside notebooks dbutils.widget. text() jobs Utilities for leveraging jobs features dbutils.jobs.taskValues. set()\n",
      "Extracted Keywords from page_13_img_1.png (Page 13):\n",
      "utility description example fs manipulates, leveraging jobs features dbutils, databricks filesystem, console dbutils, get bound value, control flow\n",
      "Generated Tags: utility description example fs manipulates, leveraging jobs features dbutils, databricks filesystem, console dbutils, get bound value\n",
      "Generated image description: graphical user interface, application, table\n",
      "Data from page_13_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_13_img_2.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_13_img_2.png (Page 13):\n",
      "ID Course 1 1 Databricks 2 2 Azure 3 3 Big_Data\n",
      "Extracted Keywords from page_13_img_2.png (Page 13):\n",
      "\n",
      "Generated Tags: \n",
      "Generated image description: table\n",
      "Data from page_13_img_2.png stored successfully.\n",
      "\n",
      "Processing image: page_14_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_14_img_1.png (Page 14):\n",
      "Admin workflow User workflow in Merge workflow in Git Production job workflow in Databricks provider Databricks Set up top-level Clone remote Repos folders repository to user Pull request and API call brings Repo in (example: folder review process Production folder to Production) latest version Create new branch based on main branch Merge into main branch Set up Git automation to Run Databricks job update Repos on Create and edit code based on Repo in merge Git automation calls Production folder Databricks Repos API Steps in Databricks Commit and push to feature branch Steps in your Git provider\n",
      "Extracted Keywords from page_14_img_1.png (Page 14):\n",
      "merge git automation calls production folder databricks repos api steps, level clone remote repos folders repository, folder review process production folder, latest version create new branch based, databricks provider databricks set, run databricks job update repos, admin workflow user workflow, git production job workflow, feature branch steps, api call brings repo, main branch merge, databricks commit, user pull request, edit code based, main branch set, git automation, merge workflow, git provider\n",
      "Generated Tags: merge git automation calls production folder databricks repos api steps, level clone remote repos folders repository, folder review process production folder, latest version create new branch based, databricks provider databricks set\n",
      "Generated image description: diagram\n",
      "Data from page_14_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_16_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_16_img_1.png (Page 16):\n",
      "Metastore Catalog Schema (Database) Table View Function\n",
      "Extracted Keywords from page_16_img_1.png (Page 16):\n",
      "metastore catalog schema, table view function\n",
      "Generated Tags: metastore catalog schema, table view function\n",
      "Generated image description: diagram\n",
      "Data from page_16_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_16_img_2.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_16_img_2.png (Page 16):\n",
      "Metastore Catalog Storage credential Schema External location Managed table External table Metastore storage External storage\n",
      "Extracted Keywords from page_16_img_2.png (Page 16):\n",
      "metastore catalog storage credential schema external location managed table external table metastore storage external storage\n",
      "Generated Tags: metastore catalog storage credential schema external location managed table external table metastore storage external storage\n",
      "Generated image description: diagram\n",
      "Data from page_16_img_2.png stored successfully.\n",
      "\n",
      "Processing image: page_17_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_17_img_1.png (Page 17):\n",
      "SELECT * FROM json. `${DA. paths. kafka_events}' SELECT * FROM text. '${DA. paths . kafka_events}' 1, offset 1- partition 1ª, timestamp Pe topic 9 value 1 WIE MD4WMDAXMDEROTOWNTQ= 219255030 1593880885085 clickstream 1 219255043 1593880892303 clickstreaffi > 2 219255106 1593880889174 clickstream 219255115 1593880888/25 clickstream YUE= MDAWMDAXMDczOUIyM2M= 219438025 1593880886108 clickstream 5 YUE=MDAWMDAXMD:200lyMzM= 219435009 1693880886106 clickstream 7 YUE= MDAWMDAXMDczOTOWMzc= 219438089 1693880887540 clickstream 7 2\n",
      "Extracted Keywords from page_17_img_1.png (Page 17):\n",
      "kafka_events }' select, `${ da, '${ da\n",
      "Generated Tags: kafka_events }' select, `${ da, '${ da\n",
      "Generated image description: table\n",
      "Data from page_17_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_17_img_2.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_17_img_2.png (Page 17):\n",
      "SELECT * FROM binaryFile. `${DA. paths. kafka_events}' + (2) Spark Jobs Table v + New result table: ON v Q Search A c path Eo modificationTime 1 > dbfs:/mnt/dbacademy-datasets/data-engineer-learning-path/v02/ecommerce/ra ... 2024-03-25T09:47:45.000+00:00 2 > dbfs:/mnt/dbacademy-datasets/data-engineer-learning-path/v02/ecommerce/ra ... 2024-03-25T09:47:46.000+00:00 3 > dbfs:/mnt/dbacademy-datasets/data-engineer-learning-path/v02/ecommerce/ra ... 2024-03-25T09:47:47.000+00:00 4 > dbfs:/mnt/dbacademy-datasets/data-engineer-learning-path/v02/ecommerce/ra ... 2024-03-25T09:47:47.000+00:00 5 > dbfs:/mnt/dbacademy-datasets/data-engineer-learning-path/v02/ecommerce/ra ... 2024-03-25T09:47:48.000+00:00\n",
      "Extracted Keywords from page_17_img_2.png (Page 17):\n",
      "spark jobs table v, new result table, kafka_events }', dbfs :/ mnt, v q search, `${ da\n",
      "Generated Tags: spark jobs table v, new result table, kafka_events }', dbfs :/ mnt, v q search\n",
      "Generated image description: text\n",
      "Data from page_17_img_2.png stored successfully.\n",
      "\n",
      "Processing image: page_19_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_19_img_1.png (Page 19):\n",
      "DESCRIBE EXTENDED sales_csv Table v + New result table: ON v Search Ac col_name Ac data_type A c comment 1 order_id bigint null 2 email string null 3 transactions_timestamp bigint null 4 total_item_quantity int null 5 purchase_revenue_in_usd double null\n",
      "Extracted Keywords from page_19_img_1.png (Page 19):\n",
      "describe extended sales_csv table v, v search ac col_name ac data_type, new result table\n",
      "Generated Tags: describe extended sales_csv table v, v search ac col_name ac data_type, new result table\n",
      "Generated image description: table\n",
      "Data from page_19_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_1_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_1_img_1.png (Page 1):\n",
      "IT\n",
      "Extracted Keywords from page_1_img_1.png (Page 1):\n",
      "\n",
      "Generated Tags: \n",
      "Generated image description: logo\n",
      "Data from page_1_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_21_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_21_img_1.png (Page 21):\n",
      "SELECT *, date_format( first_touch, \"MMM d, yyyy\") AS first_touch_date, date_format ( first_touch, \"HH:mm: ss\") AS first_touch_time, regexp_extract (email, \"( ?<= @).+\", 0) AS email_domain FROM ( SELECT *, CAST (user_first_touch_timestamp / 1e6 AS timestamp) AS first_touch FROM deduped_users\n",
      "Extracted Keywords from page_21_img_1.png (Page 21):\n",
      "select *, date_format, select *, cast\n",
      "Generated Tags: select *, date_format, select *, cast\n",
      "Generated image description: graphical user interface, text\n",
      "Data from page_21_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_21_img_2.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_21_img_2.png (Page 21):\n",
      "key value UA000000107384208 {\"device\":\"macOS\",\"ecommerce\": 0,\"event_name\":\"checkout\",\"event_previous_timestamp\": 1593880801027797,\"event_timestamp\":1593880822506642, \"geo\":{\"city\":\"Traverse City\",\"state\":\"MI\"}, \"items\":[{\"item_id\":\"M_STAN_T\",\"item_name\":\"Standard Twin Mattress\",\"item_revenue_in_usd\":595.0,\"price_in_usd\":595.0,\"quantity\": 1}],\"traffic_source\":\"google\",\"user_first_touch_t imestamp\": 1593879413256859,\"user_id\":\"UA000000107384208\"} UA000000107388621 {\"device\":\"Windows\",\"ecommerce\": \",\"event_name\":\"email_coupon\",\"event_previous_timestamp\": 1593880770092554,\"event_timestamp\":159388082932 0848,\"geo\":{\"city\":\"Hickory\",\"state\":\"NC\"},\"items\": [{\"coupon\":\"NEWBED10\",\"item_id\":\"M_STAN_F\",\"item_name\":\"Standard Full Mattress\",\"item_revenue_in_usd\":850.5,\"price_in_usd\":945.0,\"quantity\": 1}],\"traffic_source\":\"direct\",\"user_first_touch_ti mestamp\": 1593879889503719,\"user_id\":\"UA000000107388621\"}\n",
      "Extracted Keywords from page_21_img_2.png (Page 21):\n",
      "geo \":{\" city \":\" traverse city \",\" state \":\" mi \"},\n",
      "Generated Tags: geo \":{\" city \":\" traverse city \",\" state \":\" mi \"},\n",
      "Generated image description: text\n",
      "Data from page_21_img_2.png stored successfully.\n",
      "\n",
      "Processing image: page_24_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_24_img_1.png (Page 24):\n",
      "LEFT JOIN ANTI LEFT JOIN SELECT * SELECT * Everything on the left FROM TABLE_1 FROM TABLE_1 + LEFT JOIN TABLE_2 Everything on the left LEFT JOIN TABLE_2 anything on the right that ON TABLE_1.KEY = TABLE_2.KEY that is NOT on the right ON TABLE_1.KEY = TABLE_2.KEY matches WHERE TABLE_2.KEY IS NULL RIGHT JOIN ANTI RIGHT JOIN SELECT * SELECT * FROM TABLE_1 FROM TABLE_1 Everything on the right + RIGHT JOIN TABLE_2 Everything on the right RIGHT JOIN TABLE_2 anything on the left that matches ON TABLE_1.KEY = TABLE_2.KEY that is NOT on the left ON TABLE_1.KEY = TABLE_2.KEY WHERE TABLE_1.KEY IS NULL OUTER JOIN ANTI OUTER JOIN SELECT * SELECT * FROM TABLE_1 Everything on the right FROM TABLE_1 OUTER JOIN TABLE_2 OUTER JOIN TABLE_2 Everything on the left and right that is unique to each side ON TABLE_1.KEY = TABLE_2.KEY + Everything on the left ON TABLE_1.KEY = TABLE_2.KEY WHERE TABLE_1.KEY IS NULL OR TABLE_2.KEY IS NULL INNER JOIN CROSS JOIN SELECT * FROM TABLE_1 SELECT * Only the things that match on the INNER JOIN TABLE_2 All combination of rows from the FROM TABLE_1 left AND the right ON TABLE_1.KEY = TABLE_2.KEY right and the left (cartesean product) CROSS JOIN TABLE_2\n",
      "Extracted Keywords from page_24_img_1.png (Page 24):\n",
      "null outer join anti outer join select, null right join anti right join select, null inner join cross join select, left join anti left join select, cartesean product, key matches, key right\n",
      "Generated Tags: null outer join anti outer join select, null right join anti right join select, null inner join cross join select, left join anti left join select, cartesean product\n",
      "Generated image description: diagram\n",
      "Data from page_24_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_24_img_2.jpeg\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_24_img_2.jpeg (Page 24):\n",
      "UNPIVOT Example Temporary Table #CourseSales Course Earning PIVOT Query on Course Column Year .NET 2012 10000.00 SELECT * FROM #CourseSales lava 2012 20000.00 PIVOT(SUM(Earning) .NET 2012 5000.00 FOR Course IN ([.NET], Java) ) PIVOT Result .NET 2013 48000.00 AS PVTTable with Couse as Java 2013 30000.00 columns Year .NET Java 2012 15000.00 20000.00 2013 48000.00 30000.00 UNPIVOT Result UNPIVOT Query on Course Column Course Year Earning SELECT * .NET 2012 15000.00 FROM #CourseSalesPivotResult Java 2012 20000.00 UNPIVOT (Earning .NET 2013 48000.00 FOR Course IN ([.NET], Java) ) Java 2013 30000.00 AS UNPVTTable\n",
      "Extracted Keywords from page_24_img_2.jpeg (Page 24):\n",
      "course column course year earning select, coursesales course earning pivot query, unpivot example temporary table, course column year, pivot result\n",
      "Generated Tags: course column course year earning select, coursesales course earning pivot query, unpivot example temporary table, course column year, pivot result\n",
      "Generated image description: diagram\n",
      "Data from page_24_img_2.jpeg stored successfully.\n",
      "\n",
      "Processing image: page_25_img_1.png\n",
      "An error occurred while processing page_25_img_1.png: Operation returned an invalid status code 'Bad Request'. Skipping this image.\n",
      "\n",
      "Processing image: page_32_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_32_img_1.png (Page 32):\n",
      "sales customer_id spend units a1 28.94 7 a3 874.12 23 a4 8.99 1 favorite_stores customer_id store_id a1 S1 a2 s1 la4 s2\n",
      "Extracted Keywords from page_32_img_1.png (Page 32):\n",
      "\n",
      "Generated Tags: \n",
      "Generated image description: table\n",
      "Data from page_32_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_33_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_33_img_1.png (Page 33):\n",
      "DELTA LAKE\n",
      "Extracted Keywords from page_33_img_1.png (Page 33):\n",
      "delta lake\n",
      "Generated Tags: delta lake\n",
      "Generated image description: logo, company name\n",
      "Data from page_33_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_34_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_34_img_1.png (Page 34):\n",
      "%python tbl_path = f\"{DA. paths. working_dir}/external_table\" files = dbutils. fs. ls(tbl_path) display ( files) > (2) Spark Jobs Table + New result table: ON v Q Search 8 Ac path A c name 1 > dbfs:/mnt/dbacademy-users/jeanne.guilbaud@solita.fi/data-engineer-learning-p ... _delta_log/ 2 > dbfs:/mnt/dbacademy-users/jeanne.guilbaud@solita.fi/data-engineer-learning-p ... > part-00000-d9442bd9-bbbe-4cea-94ef-6daa170a52b0-c000.snappy.parq 3 > dbfs:/mnt/dbacademy-users/jeanne.guilbaud@solita.fi/data-engineer-learning-p ... > part-00001-5e95b3c9-c23a-4a5e-9181-9548d0313039-c000.snappy.par 4 > dbfs:/mnt/dbacademy-users/jeanne.guilbaud@solita.fi/data-engineer-learning-p ... > part-00002-df6bc1c2-3e60-48ea-8771-e1548d4658f5-c000.snappy.parq 5 > dbfs:/mnt/dbacademy-users/jeanne.guilbaud@solita.fi/data-engineer-learning-p ... > part-00003-0feb68da-7e24-463e-b63e-d85e84dd8d90-c000.snappy.parc\n",
      "Extracted Keywords from page_34_img_1.png (Page 34):\n",
      "working_dir }/ external_table, spark jobs table, new result table, p ... _delta_log, python tbl_path, dbfs :/ mnt, f \"{ da\n",
      "Generated Tags: working_dir }/ external_table, spark jobs table, new result table, p ... _delta_log, python tbl_path\n",
      "Generated image description: table\n",
      "Data from page_34_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_38_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_38_img_1.png (Page 38):\n",
      "Table v + New result table: ON v . Search 123 version Lo timestamp Ac userld A c userName A c operation 1 1 2024-03-26T12:35:16.000+00:00 5743206651663164 jeanne.guilbaud@solita.fi CREATE OR REPLACE TABLE AS SELECT 2 0 2024-03-26T12:32:58.000+00:00 5743206651663164 jeanne.guilbaud@solita.fi CLONE\n",
      "Extracted Keywords from page_38_img_1.png (Page 38):\n",
      "new result table, replace table, c username, fi create, fi clone, table v\n",
      "Generated Tags: new result table, replace table, c username, fi create, fi clone\n",
      "Generated image description: table\n",
      "Data from page_38_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_40_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_40_img_1.png (Page 40):\n",
      "Ac path A c name 1 > dbfs:/mnt/dbacademy-users/jeanne.guilbaud@solita.fi/data-engineer-learning-p ... 00000000000000000000.crc 2 > dbfs:/mnt/dbacademy-users/jeanne.guilbaud@solita.fi/data-engineer-learning-p ... 00000000000000000000.json 3 > dbfs:/mnt/dbacademy-users/jeanne.guilbaud@solita.fi/data-engineer-learning-p ... 00000000000000000001.00000000000000000006.compacted.json 4 > dbfs:/mnt/dbacademy-users/jeanne.guilbaud@solita.fi/data-engineer-learning-p ... 00000000000000000001.crc 5 > dbfs:/mnt/dbacademy-users/jeanne.guilbaud@solita.fi/data-engineer-learning-p ... 00000000000000000001.json 6 > dbfs:/mnt/dbacademy-users/jeanne.guilbaud@solita.fi/data-engineer-learning-p ... 00000000000000000002.crc 7 > dbfs:/mnt/dbacademy-users/jeanne.guilbaud@solita.fi/data-engineer-learning-p ... 00000000000000000002.json 8 > dbfs:/mnt/dbacademy-users/jeanne.guilbaud@solita.fi/data-engineer-learning-p ... 00000000000000000003.crc\n",
      "Extracted Keywords from page_40_img_1.png (Page 40):\n",
      "dbfs :/ mnt, ac path\n",
      "Generated Tags: dbfs :/ mnt, ac path\n",
      "Generated image description: table\n",
      "Data from page_40_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_41_img_1.png\n",
      "An error occurred while processing page_41_img_1.png: Operation returned an invalid status code 'Bad Request'. Skipping this image.\n",
      "\n",
      "Processing image: page_42_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_42_img_1.png (Page 42):\n",
      "Improve Data Quality OXO Raw Data Bronze Silver Gold Batch BI Raw Filtered, Cleaned, Business-Level Integration Augmented Aggregates Streaming ML \"Landing zone\" for raw data, no Define structure, enforce schema, Deliver continuously updated, clean schema needed evolve schema as needed data to downstream users and apps\n",
      "Extracted Keywords from page_42_img_1.png (Page 42):\n",
      "improve data quality oxo raw data bronze silver gold batch bi raw filtered, level integration augmented aggregates streaming ml, clean schema needed evolve schema, deliver continuously updated, define structure, downstream users, enforce schema, landing zone, needed data, raw data\n",
      "Generated Tags: improve data quality oxo raw data bronze silver gold batch bi raw filtered, level integration augmented aggregates streaming ml, clean schema needed evolve schema, deliver continuously updated, define structure\n",
      "Generated image description: diagram\n",
      "Data from page_42_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_47_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_47_img_1.png (Page 47):\n",
      "Python SQL Notes Python API Proprietary SQL API No syntax check Has syntax In Python, if you run a DLT notebook cell on its own it will show checks in error, whereas in SQL it will check if the command is syntactically valid and tell you. In both cases, individual notebook cells are not supposed to be run for DLT pipelines. A note on imports None The dlt module should be explicitly imported into your Python notebook libraries. In SQL, this is not the case. Tables as DataFrames Tables as query The Python DataFrame API allows for multiple transformations results of a dataset by stringing multiple API calls together. Compared to SQL, those same transformations must be saved in temporary tables as they are transformed. @dlt.table() SELECT In SQL, the core logic of your query, containing transformations statement you make to your data, is contained in the SELECT statement. In Python, data transformations are specified when you configure options for @dlt.table(). @dlt.table(comment = COMMENT This is how you add comments and table properties in Python \"Python \"SQL comment\" VS. SQL comment\",table_properties TBLPROPERTIES = {\"quality\": \"silver\"}) (\"quality\" = \"silver\")\n",
      "Extracted Keywords from page_47_img_1.png (Page 47):\n",
      "sql comment \", table_properties tblproperties, containing transformations statement, multiple transformations results, python dataframe api allows, transformations must, data transformations, syntactically valid, explicitly imported, dataframes tables, configure options, temporary tables, select statement, table properties, {\" quality \":, dlt pipelines, add comments, syntax check, sql comment, show checks, dlt module, core logic\n",
      "Generated Tags: sql comment \", table_properties tblproperties, containing transformations statement, multiple transformations results, python dataframe api allows, transformations must\n",
      "Generated image description: table\n",
      "Data from page_47_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_47_img_2.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_47_img_2.png (Page 47):\n",
      "{UPDATE} {DELETE} { INSERT } APPLY CHANGES INTO Up-to-date Snapshot\n",
      "Extracted Keywords from page_47_img_2.png (Page 47):\n",
      "date snapshot, apply changes\n",
      "Generated Tags: date snapshot, apply changes\n",
      "Generated image description: diagram\n",
      "Data from page_47_img_2.png stored successfully.\n",
      "\n",
      "Processing image: page_47_img_3.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_47_img_3.png (Page 47):\n",
      "city_updates {\"id\": 1, \"ts\": 100, \"city\": \"Bekerly, CA\"} {\"id\": 1, \"ts\": 200, \"city\": \"Berkeley, CA\"} cities id city 1 -Bekerly, GA- Berkeley, CA\n",
      "Extracted Keywords from page_47_img_3.png (Page 47):\n",
      "city \":\n",
      "Generated Tags: city \":\n",
      "Generated image description: table\n",
      "Data from page_47_img_3.png stored successfully.\n",
      "\n",
      "Processing image: page_49_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_49_img_1.png (Page 49):\n",
      "Configuration my_etl.input_path s3://my-data/json/ Add configuration CREATE STREAMING LIVE TABLE data AS SELECT * FROM cloud_files(\"${my_etl. input_path]\", \"json\") @dlt.table def data(): input_path = spark.conf.get( \"my_etl. input_path\") spark. readStream. format( \"cloud_files\") . load(input_path)\n",
      "Extracted Keywords from page_49_img_1.png (Page 49):\n",
      "add configuration create streaming live table data, configuration my_etl, input_path ]\",\n",
      "Generated Tags: add configuration create streaming live table data, configuration my_etl, input_path ]\",\n",
      "Generated image description: graphical user interface, text, application, email\n",
      "Data from page_49_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_49_img_2.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_49_img_2.png (Page 49):\n",
      "Workflows > Delta Live Tables > jeanne-guilbaud-ohff-da-delp-pipeline-demo-pipeline_demo: Example Pipeline ... Development Production Settings Schedule Start V 27/03/2024 14:26:35 . Completed Select tables for refresh Graph List > orders_bronze X Details Data quality Schema Flows Filter by flow name orders_bronze Flow ID 8b48c673-048a-40b4-b06f-04ffa8e2d58c Flow type Incremental Materialized view :: Flow status Completed Streaming table Streaming table :: orders_silver orders_by_date Start time 27/03/2024 14:26:39 orders_bronze Completed - 2s Completed . 1s Completed . 4s Duration 2s . - . - .. . . .7 .0 + - All Info Warning Error 2 Filter ... × X 17 minutes ago user_action User jeanne.guilbaud@solita.fi started an update. 17 minutes ago create_update Update 474e93 started by API_CALL. 17 minutes ago update_progress Update 474e93 is INITIALIZING. 17 minutes ago update_progress Update 474e93 is SETTING_UP_TABLES. Show new events 17 minutes ago flow_definition Flow 'orders_bronze' defined as INCREMENTAL.\n",
      "Extracted Keywords from page_49_img_2.png (Page 49):\n",
      "orders_bronze x details data quality schema flows filter, completed select tables, refresh graph list, delta live tables, fi started\n",
      "Generated Tags: orders_bronze x details data quality schema flows filter, completed select tables, refresh graph list, delta live tables, fi started\n",
      "Generated image description: graphical user interface, text\n",
      "Data from page_49_img_2.png stored successfully.\n",
      "\n",
      "Processing image: page_4_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_4_img_1.png (Page 4):\n",
      "Data Science ETL & Orchestration Data & Al Real-time Analytics Warehousing Databricks Al Delta Live Tables Workflows Databricks SQL Unified security, governance, and cataloging Unity Catalog Unified data storage for reliability and sharing Delta Lake Open Data Lake All Raw Data (Logs, Texts, Audio, Video, Images)\n",
      "Extracted Keywords from page_4_img_1.png (Page 4):\n",
      "time analytics warehousing databricks al delta live tables workflows databricks sql unified security, cataloging unity catalog unified data storage, sharing delta lake open data lake, orchestration data, data science etl, raw data, al real\n",
      "Generated Tags: time analytics warehousing databricks al delta live tables workflows databricks sql unified security, cataloging unity catalog unified data storage, sharing delta lake open data lake, orchestration data, data science etl\n",
      "Generated image description: graphical user interface, text, application\n",
      "Data from page_4_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_54_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_54_img_1.png (Page 54):\n",
      "Delta Live Tables Workflow Jobs Notebooks only JARs, notebooks, DLT, application written in Source Scala, Java, Python Dependencies Automatically determined Manually set Cluster Self-provisioned Self-provisioned or existing Timeouts and Retries Not supported Supported Import Libraries Not supported Supported\n",
      "Extracted Keywords from page_54_img_1.png (Page 54):\n",
      "python dependencies automatically determined manually set cluster self, supported supported import libraries, application written, supported supported, existing timeouts, provisioned self, source scala\n",
      "Generated Tags: python dependencies automatically determined manually set cluster self, supported supported import libraries, application written, supported supported, existing timeouts\n",
      "Generated image description: table\n",
      "Data from page_54_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_54_img_2.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_54_img_2.png (Page 54):\n",
      "NO CAN CAN MANAGE IS CAN Ability PERMISSIONS VIEW RUN OWNER MANAGE View job details and settings x x X x View results x X x x View Spark Ul, logs of a job run X X X Run now X x X Cancel run x X X Edit job settings X X Delete job X x Modify permissions X X\n",
      "Extracted Keywords from page_54_img_2.png (Page 54):\n",
      "x x x cancel run x x x edit job settings x x delete job x x modify permissions x x, ability permissions view run owner manage view job details, settings x x x x view results x x x x view spark ul, job run x x x run\n",
      "Generated Tags: x x x cancel run x x x edit job settings x x delete job x x modify permissions x x, ability permissions view run owner manage view job details, settings x x x x view results x x x x view spark ul, job run x x x run\n",
      "Generated image description: table\n",
      "Data from page_54_img_2.png stored successfully.\n",
      "\n",
      "Processing image: page_55_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_55_img_1.png (Page 55):\n",
      "Depends on Query-logs X IV Advanced options > Cancel Save task\n",
      "Extracted Keywords from page_55_img_1.png (Page 55):\n",
      "logs x iv advanced options, cancel save task\n",
      "Generated Tags: logs x iv advanced options, cancel save task\n",
      "Generated image description: graphical user interface, application\n",
      "Data from page_55_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_55_img_2.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_55_img_2.png (Page 55):\n",
      "Sequence Funnel Fan-out\n",
      "Extracted Keywords from page_55_img_2.png (Page 55):\n",
      "sequence funnel fan\n",
      "Generated Tags: sequence funnel fan\n",
      "Generated image description: chart, box and whisker chart\n",
      "Data from page_55_img_2.png stored successfully.\n",
      "\n",
      "Processing image: page_56_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_56_img_1.png (Page 56):\n",
      "Run duration Mar 15, 13:22 Mar 3, 14:22 Ma' 3m 44s 1m 52s Build_Features Clicks_Ingest_flaky Match Tasks Orders_Ingest Persist_Features Sessionize Train Job run\n",
      "Extracted Keywords from page_56_img_1.png (Page 56):\n",
      "\n",
      "Generated Tags: \n",
      "Generated image description: diagram\n",
      "Data from page_56_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_59_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_59_img_1.png (Page 59):\n",
      "Data Lake Data analyst ... Unity Catalog Metadata Data engineer Data Warehouse Data scientist ML and Al\n",
      "Extracted Keywords from page_59_img_1.png (Page 59):\n",
      "unity catalog metadata data engineer data warehouse data scientist ml, data lake data analyst ...\n",
      "Generated Tags: unity catalog metadata data engineer data warehouse data scientist ml, data lake data analyst ...\n",
      "Generated image description: a group of boxes\n",
      "Data from page_59_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_5_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_5_img_1.png (Page 5):\n",
      "Data Science ETL & Orchestration Data & AI Real-time Analytics Warehousing Databricks Al Delta Live Tables Workflows Databricks SQL Use generative Al to understand the semantics of your data Data Intelligence Engine Unified security, governance, and cataloging Unity Catalog Unified data storage for reliability and sharing Delta Lake Open Data Lake All Raw Data (Logs, Texts, Audio, Video, Images)\n",
      "Extracted Keywords from page_5_img_1.png (Page 5):\n",
      "time analytics warehousing databricks al delta live tables workflows databricks sql use generative al, data data intelligence engine unified security, cataloging unity catalog unified data storage, sharing delta lake open data lake, orchestration data, data science etl, raw data, ai real\n",
      "Generated Tags: time analytics warehousing databricks al delta live tables workflows databricks sql use generative al, data data intelligence engine unified security, cataloging unity catalog unified data storage, sharing delta lake open data lake, orchestration data\n",
      "Generated image description: graphical user interface, application\n",
      "Data from page_5_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_60_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_60_img_1.png (Page 60):\n",
      "Metastore Catalogs Schemas External Managed tables tables Views\n",
      "Extracted Keywords from page_60_img_1.png (Page 60):\n",
      "metastore catalogs schemas external managed tables tables views\n",
      "Generated Tags: metastore catalogs schemas external managed tables tables views\n",
      "Generated image description: diagram\n",
      "Data from page_60_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_60_img_2.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_60_img_2.png (Page 60):\n",
      "Metastore hive_metastore Catalog 1 Catalog 2 unity Workspace Schema (Database) catalog Table View Function\n",
      "Extracted Keywords from page_60_img_2.png (Page 60):\n",
      "catalog table view function\n",
      "Generated Tags: catalog table view function\n",
      "Generated image description: diagram\n",
      "Data from page_60_img_2.png stored successfully.\n",
      "\n",
      "Processing image: page_60_img_3.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_60_img_3.png (Page 60):\n",
      "Traditional SQL two-level Unity Catalog three-level namespace namespace ... ... SELECT * FROM schema.table SELECT * FROM catalog.schema.table\n",
      "Extracted Keywords from page_60_img_3.png (Page 60):\n",
      "level namespace namespace ... ..., level unity catalog three, traditional sql two, table select\n",
      "Generated Tags: level namespace namespace ... ..., level unity catalog three, traditional sql two, table select\n",
      "Generated image description: radar chart\n",
      "Data from page_60_img_3.png stored successfully.\n",
      "\n",
      "Processing image: page_62_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_62_img_1.png (Page 62):\n",
      "Data Provider Data Recipient Access Power BI permissions DELTA SHARING Spark: *el pandas = Delta Sharing Protocol Delta Lake Table Delta Sharing Server tableau .. Any Sharing Client\n",
      "Extracted Keywords from page_62_img_1.png (Page 62):\n",
      "data provider data recipient access power bi permissions delta sharing spark, delta sharing protocol delta lake table delta sharing server tableau .., sharing client, el pandas\n",
      "Generated Tags: data provider data recipient access power bi permissions delta sharing spark, delta sharing protocol delta lake table delta sharing server tableau .., sharing client, el pandas\n",
      "Generated image description: graphical user interface, application\n",
      "Data from page_62_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_62_img_2.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_62_img_2.png (Page 62):\n",
      "Before Unity Catalog With Unity Catalog Workspace 1 Workspace 2 Unity Catalog User/group User/group User/group Metastore Access management management management controls Metastore Metastore Access controls Access controls Workspace 1 Workspace 2 Compute Compute Compute Compute resources resources resources resources\n",
      "Extracted Keywords from page_62_img_2.png (Page 62):\n",
      "unity catalog, group user\n",
      "Generated Tags: unity catalog, group user\n",
      "Generated image description: diagram\n",
      "Data from page_62_img_2.png stored successfully.\n",
      "\n",
      "Processing image: page_63_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_63_img_1.png (Page 63):\n",
      "2 Check namespace, metadata and grants unity 0-0- catalog 1 Send query 4 Return short-lived token and signed URL Audit Log O Assume IAM Role or Principal 3 8 Send result 5 Request data from URL Service Principal with short-lived token Compute 7 Enforce policies 6 Return data Cloud Storage\n",
      "Extracted Keywords from page_63_img_1.png (Page 63):\n",
      "url service principal, signed url audit log, assume iam role, lived token\n",
      "Generated Tags: url service principal, signed url audit log, assume iam role, lived token\n",
      "Generated image description: diagram\n",
      "Data from page_63_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_64_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_64_img_1.png (Page 64):\n",
      "Supported Init scripts Access mode Legacy Credential languages table ACL passthrough Shareable RDD API DBFS Fuse and Dynamic Machine mounts libraries views learning No Isolation Shared All Single user All Shared SQL Python\n",
      "Extracted Keywords from page_64_img_1.png (Page 64):\n",
      "supported init scripts access mode legacy credential languages table acl passthrough shareable rdd api dbfs fuse, dynamic machine mounts libraries views learning, shared sql python, isolation shared, single user\n",
      "Generated Tags: supported init scripts access mode legacy credential languages table acl passthrough shareable rdd api dbfs fuse, dynamic machine mounts libraries views learning, shared sql python, isolation shared, single user\n",
      "Generated image description: chart\n",
      "Data from page_64_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_64_img_2.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_64_img_2.png (Page 64):\n",
      "Cloud Admin Identity Admin Account Admin Metastore Admin Data Owner Workspace Admin\n",
      "Extracted Keywords from page_64_img_2.png (Page 64):\n",
      "cloud admin identity admin account admin metastore admin data owner workspace admin\n",
      "Generated Tags: cloud admin identity admin account admin metastore admin data owner workspace admin\n",
      "Generated image description: table\n",
      "Data from page_64_img_2.png stored successfully.\n",
      "\n",
      "Processing image: page_65_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_65_img_1.png (Page 65):\n",
      "CREATE TABLE READ FILES WRITE FILES Create an External Table Read files directly using this Write files directly using directly using this Storage Storage Credential this Storage Credential Credential Storage Credential External Location Create an External Table from files governed by Read files governed by this Write files governed by this this External Location External Location External Location\n",
      "Extracted Keywords from page_65_img_1.png (Page 65):\n",
      "storage credential credential storage credential external location create, external location external location external location, create table read files write files create, write files directly using directly using, external table read files directly using, storage storage credential, write files governed, read files governed, files governed, external table\n",
      "Generated Tags: storage credential credential storage credential external location create, external location external location external location, create table read files write files create, write files directly using directly using, external table read files directly using\n",
      "Generated image description: timeline\n",
      "Data from page_65_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_6_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_6_img_1.png (Page 6):\n",
      "Stitch Qlik Q Spark® presto A Talend Company Fivetran Informatica Ingest Query amazon snowflake from from REDSHIFT StreamSets Infoworks Azure syncsort DELTA LAKE Synapse Parquet Analytics Amazon Athena Store data in amazon Google S3 Azure Cloud Storage STORAGE Alibaba Cloud\n",
      "Extracted Keywords from page_6_img_1.png (Page 6):\n",
      "stitch qlik q spark ® presto\n",
      "Generated Tags: stitch qlik q spark ® presto\n",
      "Generated image description: logo, company name\n",
      "Data from page_6_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_6_img_2.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_6_img_2.png (Page 6):\n",
      "External Compute Platforms amazon BI & Data Data Data Data ATHENA trino Warehousing Engineering Streaming Science & ML presto amazon EMR Spark - Open Interfaces Unity Catalog One security and governance model for structured/unstructured data as well as Al Cloud Data Lake Catalog Federation Data Federation All structured, semi-structured, and unstructured Azure Data data amazon snowflake Google MySQL amazon REDSHIFT Big Query Lake Storage 53 AWS Glue Azure Synapse Google Cloud Storage Analytics SQL Azure\n",
      "Extracted Keywords from page_6_img_2.png (Page 6):\n",
      "al cloud data lake catalog federation data federation, open interfaces unity catalog one security, external compute platforms amazon bi, ml presto amazon emr spark, unstructured data, governance model\n",
      "Generated Tags: al cloud data lake catalog federation data federation, open interfaces unity catalog one security, external compute platforms amazon bi, ml presto amazon emr spark, unstructured data\n",
      "Generated image description: timeline\n",
      "Data from page_6_img_2.png stored successfully.\n",
      "\n",
      "Processing image: page_6_img_3.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_6_img_3.png (Page 6):\n",
      "Databricks Unity Catalog User Access Data Automated Auditing Management Controls Lineage Monitoring Data Discovery and Classification Data (Files | Tables | Notebooks | Model Registry | Feature Store) Sharing\n",
      "Extracted Keywords from page_6_img_3.png (Page 6):\n",
      "databricks unity catalog user access data automated auditing management controls lineage monitoring data discovery, classification data, model registry, feature store\n",
      "Generated Tags: databricks unity catalog user access data automated auditing management controls lineage monitoring data discovery, classification data, model registry, feature store\n",
      "Generated image description: a screenshot of a computer\n",
      "Data from page_6_img_3.png stored successfully.\n",
      "\n",
      "Processing image: page_8_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_8_img_1.png (Page 8):\n",
      "Users Control plane Customer - Interactive databricks databricks Data plane users Web application - Cluster Cluster Cluster - Configurations - - Bl apps Your cloud storage - Notebooks, repos, DBSQL Qlik Q looker Cluster manager Data DBFS root\n",
      "Extracted Keywords from page_8_img_1.png (Page 8):\n",
      "interactive databricks databricks data plane users web application, dbsql qlik q looker cluster manager data dbfs root, users control plane customer, cluster cluster cluster, cloud storage, bl apps\n",
      "Generated Tags: interactive databricks databricks data plane users web application, dbsql qlik q looker cluster manager data dbfs root, users control plane customer, cluster cluster cluster, cloud storage\n",
      "Generated image description: diagram\n",
      "Data from page_8_img_1.png stored successfully.\n",
      "\n",
      "Processing image: page_9_img_1.png\n",
      "Waiting for Azure OCR results...\n",
      "Extracted Text from page_9_img_1.png (Page 9):\n",
      "Workloads Cluster Worker Notebook VM instance Worker Job Driver VM instance VM instance Pipeline Worker VM instance\n",
      "Extracted Keywords from page_9_img_1.png (Page 9):\n",
      "\n",
      "Generated Tags: \n",
      "Generated image description: diagram\n",
      "Data from page_9_img_1.png stored successfully.\n"
     ]
    }
   ],
   "source": [
    "# Main processing workflow\n",
    "try:\n",
    "    pdf_path = r\"C:\\Users\\sami.arem\\OneDrive - Solita Oy\\Desktop\\cratedb\\cratebot - Copy\\data\\Databricks.pdf\"\n",
    "    output_folder = r\"C:\\Users\\sami.arem\\OneDrive - Solita Oy\\Desktop\\extracted_images\"\n",
    "\n",
    "    # Ensure output folder exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Step 1: Extract images from PDF\n",
    "    print(\"Extracting images from PDF...\")\n",
    "    image_count = extract_images_with_pymupdf(pdf_path, output_folder)\n",
    "    print(f\"Extracted {image_count} images from the PDF.\")\n",
    "\n",
    "    # Step 2: Process each image\n",
    "    for image_file in sorted(os.listdir(output_folder)):\n",
    "        if image_file.endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            image_path = os.path.join(output_folder, image_file)\n",
    "            try:\n",
    "                print(f\"\\nProcessing image: {image_file}\")\n",
    "\n",
    "                # Extract page number from the image name\n",
    "                page_number = extract_page_number(image_file)\n",
    "                if page_number is None:\n",
    "                    print(f\"Could not extract page number from {image_file}. Skipping this image.\")\n",
    "                    continue\n",
    "\n",
    "                # Extract text and keywords\n",
    "                extracted_text, keywords = analyze_image_with_azure(image_path)\n",
    "\n",
    "                # Print extracted text and keywords\n",
    "                print(f\"Extracted Text from {image_file} (Page {page_number}):\\n{extracted_text}\")\n",
    "                print(f\"Extracted Keywords from {image_file} (Page {page_number}):\\n{', '.join(keywords)}\")\n",
    "\n",
    "                # Generate tags\n",
    "                tags = generate_tags(keywords)\n",
    "                print(f\"Generated Tags: {', '.join(tags)}\")\n",
    "\n",
    "                # Generate embeddings\n",
    "                embedding_text = embeddings.embed_query(extracted_text)  # Embedding for the text\n",
    "                keyword_string = \", \".join(keywords) if keywords else \"\"\n",
    "                embedding_keywords = embeddings.embed_query(keyword_string)  # Embedding for the keywords\n",
    "\n",
    "                # Generate image description\n",
    "                image_description = generate_image_description(image_path)\n",
    "\n",
    "                # Store data in CrateDB\n",
    "                store_data_to_cratedb(\n",
    "                    page_number,\n",
    "                    image_file,\n",
    "                    extracted_text,\n",
    "                    keywords,\n",
    "                    tags,\n",
    "                    embedding_text,\n",
    "                    embedding_keywords,\n",
    "                    image_description\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing {image_file}: {e}. Skipping this image.\")\n",
    "            time.sleep(5)  # Delay between operations\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot's response:\n",
      " The Unity Catalog in Databricks is a feature that provides access management controls for users/groups, Metastore access controls, and compute resources within workspaces. It assists in user access data, automated auditing, management controls, lineage monitoring, data discovery and classification, and sharing of various data components such as files, tables, notebooks, model registry, and feature store.\n",
      "\n",
      "In essence, the Unity Catalog in Databricks serves as a centralized platform that facilitates efficient data management, access control, auditing, and sharing functionalities across different data components within the workspace environment.\n",
      "\n",
      "The information regarding the Unity Catalog was extracted from pages 6 and 62.\n"
     ]
    }
   ],
   "source": [
    "my_question = \"What is Unity catalog?\"\n",
    "query_embedding = embeddings.embed_query(my_question)  # Generate embedding for the question\n",
    "\n",
    "# KNN query to include page numbers in the result\n",
    "knn_query = \"\"\"\n",
    "    SELECT text, page_number\n",
    "    FROM text_data\n",
    "    WHERE knn_match(embedding_text, ?, 4)\n",
    "    ORDER BY _score DESC\n",
    "    LIMIT 4\n",
    "\"\"\"\n",
    "\n",
    "# Connect to CrateDB and execute the query\n",
    "documents = []\n",
    "page_numbers = []\n",
    "\n",
    "try:\n",
    "    conn = get_crate_connection()  # Get CrateDB connection\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Execute the KNN query with the embedding\n",
    "    cursor.execute(knn_query, [query_embedding])\n",
    "    \n",
    "    # Fetch and process the results\n",
    "    for record in cursor.fetchall():\n",
    "        documents.append(record[0])  # Assuming 'text' is the relevant field to return\n",
    "        page_numbers.append(record[1])  # Extract the page number\n",
    "    \n",
    "    # Combine documents and page numbers to generate context\n",
    "    context = \"\"\n",
    "    for doc, page_number in zip(documents, page_numbers):\n",
    "        context += f\"Page {page_number}:\\n{doc}\\n\\n\"\n",
    "    \n",
    "    # Define the system prompt with instructions and context\n",
    "    system_prompt = f\"\"\"\n",
    "    You are an expert in Databricks and are answering a user's question based on the following context extracted from multiple sources (images of a document). \n",
    "    Your task is to synthesize information from ALL the available images to answer the user's question comprehensively. \n",
    "\n",
    "    Key Requirements:\n",
    "    - Synthesize and combine ALL relevant information from the images that pertains to the question.\n",
    "    - Your response should provide a clear, detailed and concise answer to the user's question.\n",
    "    - At the END of your response, include a note specifying the page numbers where the referenced information was extracted. \n",
    "      For example, 'The information was extracted from pages X, Y, and Z.'\n",
    "    - if the provided context does not contain relevant information, respond only with: \"I don't know.\"\n",
    "    - If the query is not about Databricks, respond with: \"I don't know.\"\n",
    "\n",
    "    Context (summarized from all images):\n",
    "    {context}\n",
    "\n",
    "    User's Question:\n",
    "    {my_question}\n",
    "    \"\"\"\n",
    "\n",
    "    # Set your OpenAI API key here (ensure you have the key set correctly in your environment variables)\n",
    "    openai.api_key = os.getenv('OPENAI_API_KEY')  # Ensure this is set in your environment\n",
    "\n",
    "    # Create the chat completion\n",
    "    chat_completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Specify the model to use\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": my_question},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Extract and print the response\n",
    "    response = chat_completion['choices'][0]['message']['content']\n",
    "    print(\"Chatbot's response:\\n\", response)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while querying CrateDB: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
